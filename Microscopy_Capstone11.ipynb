{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzLL_7dMmXuG"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAmKm5e_wvLL"
      },
      "source": [
        "Welcome to the **Capstone Project focused on Biological Microscopy Images**!\n",
        "\n",
        "***Before starting this notebook, make a local copy in your Google Drive by selecting File > Save a Copy in Drive.***\n",
        "\n",
        ">This notebook is designed to be modular. Each time you return to this notebook, first run the \"Introduction\" section to install and import packages, and load data and metadata. Then, you can run each subsequent section independently (including \"Exploratory Data Analysis\", \"Radiation Type Classifiers\", \"Sex Classifiers\", \"Strain Classifiers\").\n",
        "\n",
        "----\n",
        "\n",
        "In this notebook, just like in the [Bioimaging Notebook](https://colab.research.google.com/drive/1n3PyMRgfrb4fWD53fgSuhtV364qUaW4r?usp=sharing), we will be using data from the [NASA Biological and Physical Sciences Microscopy Benchmark Training dataset](https://registry.opendata.aws/bps_microscopy/), publicly available on the Amazon Web Services Open Data Registry.\n",
        "\n",
        "Recall that this dataset contains thousands of images of cellular nuclei from mouse fibroblast cells that have been irradiated with X-rays or high-energy iron (Fe) particles at different doses, and imaged at different timepoints.\n",
        "\n",
        "The cell nuclei were then stained with a fluorescent marker that adheres to areas of DNA damage and repair, called *53bp1*.\n",
        "\n",
        "Thus, when viewing the images, we can see that the brightly fluorescent spots in the cell nucleus indicate DNA damage due to radiation exposure. This allows us to study the impact of radiation exposure on the DNA within cells. These bright spots are called DNA damage *foci*.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1OS3kJDg88GPwZeFcCEKtr3eDANj4nBw_\">\n",
        "\n",
        "\n",
        "More information on this dataset is available [here](https://docs.google.com/document/d/e/2PACX-1vTIjUPenLxVX0stErsBbK884QMJW_Ur1mqHJ9K3KIZl3klT90cxHDppsEvz5Z6Skdu13X8tzghqyWcN/pub).\n",
        "\n",
        "----\n",
        "\n",
        "In this Capstone Research Project, you will be using machine learning methods to analyze the dataset. Here are some of the questions you will be answering:\n",
        "\n",
        "- What patterns are present when we cluster the dataset, and what does that tell us about the biological similarities between images in the dataset?\n",
        "- Can we train a machine learning classifier to classify radiation type (X-ray vs. Fe)?\n",
        "  - What type of classifier works best for this task? Why?\n",
        "- What do the misclassified images have in common? What about the correctly classified images?\n",
        "  - What does this tell us about what biological patterns are being used by the models?\n",
        "- Can we train a machine learning classifier to classify the *sex* of the mouse from which each cell nucleus image was taken?\n",
        "  - What about the *strain* of the mouse?\n",
        "  - If not, what does this tell us about what biological traits are being captured in the radiation images?\n",
        "\n",
        "\n",
        "After working through these questions, you will have the chance to independently pursue additional questions if you have the time and interest. These questions are completely optional, and you can also return to them after this course is finished:\n",
        "\n",
        "- Does a pre-trained image classifier perform better than a from-scratch model?\n",
        "- Can you train a regression algorithm to identify the number of DNA damage *foci* in an image?\n",
        "- Can you design a method to count the number of DNA damage *foci* in an image?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tsq5KA2TZcP"
      },
      "source": [
        "## Installing and Importing Packages\n",
        "\n",
        "First, let's install and import packages needed for reading in the data and metadata.\n",
        "\n",
        "The `s3fs` library in Python is used to mount an Amazon S3 bucket so that you can operate in files and directories in an S3 bucket like a local file system. Since our dataset is stored on the Amazon Web Services Open Data Registry, the `s3fs` library will allow us to access the dataset without downloading it.\n",
        "\n",
        "The `boto3` library is another Python library for interacting with Amazon S3 services."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4T9eZWgX1ex"
      },
      "outputs": [],
      "source": [
        "# Install s3fs library\n",
        "!pip install s3fs\n",
        "# Install boto3 library\n",
        "!pip install boto3\n",
        "# Import packages\n",
        "from botocore import UNSIGNED\n",
        "from botocore.config import Config\n",
        "import boto3\n",
        "import pandas as pd\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N2oHM-GxiQyj"
      },
      "outputs": [],
      "source": [
        "# Define a function to load image from S3 using the index\n",
        "def load_image_from_s3(index, labels_df):\n",
        "  row = labels_df.iloc[index]\n",
        "  img_filename = row['filename']\n",
        "  obj = s3_client.get_object(Bucket=s3_bucket, Key=os.path.join(s3_dir_path, img_filename))\n",
        "  img_bytes = obj['Body'].read()\n",
        "  img = Image.open(BytesIO(img_bytes))\n",
        "  return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhesU7ZQTg-i"
      },
      "source": [
        "## Read In Metadata\n",
        "\n",
        "In order to train a machine learning model on this image dataset, we need to know as much as possible about each image.\n",
        "\n",
        "Fortunately, lots of metadata has been published about each image. Let's read it in!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_me95EuLTnNF"
      },
      "source": [
        "### Metadata About the Image Files\n",
        "\n",
        "First, we'll mount the Amazon S3 bucket that contains the image data and its associated metadata table, which is called `meta.csv`. We'll use the `pandas` library to read in the metadata table, perform some filtering, and take a look at the final table.\n",
        "\n",
        "\n",
        "> *Filtering Steps:*\n",
        "In order to reduce compute times, we will filter the metadata to only include images that were taken 4 hours after radiation exposure (when the DNA damage signal is strongest), and we will select a random 10% subset of those images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GIsR1_KnLdEE"
      },
      "outputs": [],
      "source": [
        "# Mount the S3 bucket\n",
        "#s3 = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))\n",
        "\n",
        "# Define the bucket and directory within the bucket to our data\n",
        "s3_bucket = 'nasa-bps-training-data'\n",
        "s3_dir_path = \"Microscopy/train\"\n",
        "\n",
        "# Define the directory path as a string and convert to os.PathLike object\n",
        "image_dir = os.fspath(f\"s3://{s3_bucket}/{s3_dir_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_z_5e-lXtzH"
      },
      "outputs": [],
      "source": [
        "# Load labels from CSV\n",
        "meta = pd.read_csv(image_dir+'/meta.csv')\n",
        "\n",
        "# Get only the 4hr timepoint images for the radiation classifiers\n",
        "meta_4hr = meta[(meta['hr_post_exposure'] == 4)]\n",
        "\n",
        "# Sample 10% from each class to limit compute needs\n",
        "sub_meta_4hr = meta_4hr.groupby('particle_type').apply(lambda x: x.sample(frac=0.1, random_state=42))\n",
        "sub_meta_4hr = sub_meta_4hr.reset_index(drop=True) # Reset index to avoid multi-index\n",
        "\n",
        "meta = sub_meta_4hr\n",
        "\n",
        "# Pull out plate and well from filename into their own column to use for mapping later\n",
        "meta['plate_well'] = [x.split('_')[0] + '_' + x.split('-')[1].split('_')[0] for x in meta['filename']]\n",
        "\n",
        "meta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5gu00QcSZP5"
      },
      "source": [
        "Take a look at the the `meta.csv` table, which has 5 different columns with information about each image.\n",
        "\n",
        "**CHALLENGE QUESTIONS:**\n",
        "Take a look at the [documentation](https://docs.google.com/document/d/e/2PACX-1vTIjUPenLxVX0stErsBbK884QMJW_Ur1mqHJ9K3KIZl3klT90cxHDppsEvz5Z6Skdu13X8tzghqyWcN/pub) for this dataset. What do each of the columns tell us?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9odEC-7HUFfV"
      },
      "source": [
        "**Double click here to enter your answers to the questions in this text box.**\n",
        "<font color=\"green\" size=5>\n",
        "Answers:\n",
        "<br>\n",
        "<ol>\n",
        "<li>\n",
        "\"filename\"\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\"dose_Gy\"\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\"particle_type\"\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\"hr_post_exposure\"\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\"plate_well\"\n",
        "</li>\n",
        "</ol>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe24C8OCTq5m"
      },
      "source": [
        "### Metadata About the Mice\n",
        "\n",
        "Now, let's take a look at some metadata that has been published about the mice that were used in this study. The information about the mice is available through the [NASA Open Science Data Repository (OSDR)](https://osdr.nasa.gov/bio/), in study [OSD-366](https://osdr.nasa.gov/bio/repo/data/studies/OSD-366).\n",
        "\n",
        "First, let's read in all the information we have about the mice using the `pandas` library and the [OSDR Public API](https://genelab.nasa.gov/genelabAPIs) which allows us to use Python code to access the contents of OSDR studies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lTOg9X3aHrRX"
      },
      "outputs": [],
      "source": [
        "# Read in the sample metadata from OSD-366 using the OSDR Public API\n",
        "url = 'https://osdr.nasa.gov/geode-py/ws/studies/OSD-366/download?source=datamanager&file=GLDS-366_Histology_raw_pheno_V3.csv'\n",
        "\n",
        "sampleMeta = pd.read_csv(url)[['Sample Name', 'plate', 'plate_well', 'Strain', 'Gender', 'avg_foci_no_outl']]\n",
        "sampleMeta = sampleMeta.rename(columns={'plate_well': 'well', 'Gender': 'Sex'})\n",
        "sampleMeta['plate_well'] = sampleMeta['plate'] + '_' + sampleMeta['well']\n",
        "sampleMeta.drop(['Sample Name', 'plate', 'well'], axis=1, inplace=True)\n",
        "sampleMeta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csQcqqcwVCLS"
      },
      "source": [
        "Take a look at the table above. The different columns in this table tell us lots of information about the mice:\n",
        "- \"plate_well\" = the plate and well number (remember each well in a 96-well plate contains cells from one mouse)\n",
        "- \"Strain\" = the strain of the mouse\n",
        "- \"Sex\" = the sex of the mouse\n",
        "- \"avg_foci_no_outl\" = the number of DNA damage foci found in that well\n",
        "\n",
        "**CHALLENGE QUESTION:** If we want to combine the metadata about the mice with the metadata about the images, is there a column that both of these tables have in common that we could use to map between the tables? If so, what is it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxE4xAP6XybH"
      },
      "source": [
        "**Double click here to enter your answers to the questions in this text box.**\n",
        "\n",
        "<font color=\"green\" size=5>\n",
        "Answer:\n",
        "<br>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjZUlZrpY6sx"
      },
      "source": [
        "----\n",
        "Now, let's combine the metadata about the images with the metadata about the mice, so that we have a single metadata table with all the information we need to train machine learning models.\n",
        "\n",
        "We will use Python *dictionaries* for this task in the code blocks below. We will use the \"plate-well\" column in each table to map the two tables together."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LQacaOrYHZS"
      },
      "outputs": [],
      "source": [
        "# Create dictionaries that relate plate-well and sex or strain\n",
        "sexDict = sampleMeta[['plate_well', 'Sex']].set_index('plate_well').to_dict()['Sex']\n",
        "strainDict = sampleMeta[['plate_well', 'Strain']].set_index('plate_well').to_dict()['Strain']\n",
        "fociDict = sampleMeta[['plate_well', 'avg_foci_no_outl']].set_index('plate_well').to_dict()['avg_foci_no_outl']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stdgQdhDjqw0"
      },
      "outputs": [],
      "source": [
        "# Collect sex and strain and nfoci for all images, using the plate-well combinations\n",
        "# Add to metadata table\n",
        "\n",
        "sexMap = []\n",
        "for x in meta['plate_well']:\n",
        "  sexMap.append(sexDict[x])\n",
        "meta['sex'] = sexMap\n",
        "\n",
        "strainMap = []\n",
        "for x in meta['plate_well']:\n",
        "  strainMap.append(strainDict[x])\n",
        "meta['strain'] = strainMap\n",
        "\n",
        "fociMap = []\n",
        "for x in meta['plate_well']:\n",
        "  fociMap.append(fociDict[x])\n",
        "meta['avg_foci_no_outl'] = fociMap\n",
        "\n",
        "meta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TODqN48tZQ3k"
      },
      "source": [
        "> Nice work! We now have a single metadata table with the filename of each image, plus several different **classes** for each image to train our classifiers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjLJpXp3ZISj"
      },
      "source": [
        "## Read in the Image Data\n",
        "\n",
        "Now, we need to load the image data into memory and preprocess it in preparation for training machine learning algorithms.\n",
        "\n",
        "First, we'll import a few packages that are useful for image preprocessing such as `matplotlib` and `PIL`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fdj5vBBZKj48"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "# from sklearn.decomposition import PCA\n",
        "# from sklearn.manifold import TSNE\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "from io import BytesIO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sBO5wBClqAo"
      },
      "source": [
        "Next, we'll write a Python function which loads all the images and performs some preprocessing. This function:\n",
        "- Loads each image into memory\n",
        "- Filters out any images with pixel intensity greater than 4000. The settings on the microscope that captured these images should only allow pixel intensities up to 4000. *Any images with pixel intensities greater than 4000 indicate an error during the automated image capture process, so we filter out those low quality images.*\n",
        "- Calculates the dimensions (width, height) of each image so that we can analyze the size distribution across the dataset. *This is important because most neural network classifiers require all images to be the same size.*\n",
        "\n",
        "Then we'll use that function. **NOTE: the next 2 cells take a few minutes to run.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NknJWW57GNiF"
      },
      "outputs": [],
      "source": [
        "# Function to load and get image sizes\n",
        "def load_images(file_list):\n",
        "    images = []\n",
        "    images_resized = []\n",
        "    bad_images = []\n",
        "    for file in file_list:\n",
        "      obj = s3_client.get_object(Bucket=s3_bucket, Key=s3_dir_path+'/'+file)\n",
        "      img_data = obj['Body'].read()\n",
        "      img = Image.open(BytesIO(img_data))\n",
        "      if np.array(img).max() < 4000: # filter out any images with pixel intensity > 4000 (low quality images)\n",
        "        img_resize = img.resize((128, 128))\n",
        "        images.append(img) # for evaluating the original sizes of the images\n",
        "        images_resized.append(img_resize) # for training classifiers\n",
        "      else:\n",
        "        bad_images.append(file)\n",
        "\n",
        "    return images, np.array(images_resized), bad_images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDSombFPGNkn"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess images - this takes a few minutes\n",
        "image_files = meta['filename'].tolist()\n",
        "loaded_images, resized_images, filtered_images = load_images(image_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uw6E-0IQcpCu"
      },
      "outputs": [],
      "source": [
        "len(filtered_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JA2pK2wmhBw"
      },
      "source": [
        "The code above prints out the length of the `filtered_images` variable, which holds a list of all the images that were filtered out due to pixel intensity greater than 4000.\n",
        "\n",
        "**CHALLENGE QUESTION:** How many poor quality images were filtered out of the dataset on the basis of high pixel intensity?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HatxSfGrmtvK"
      },
      "source": [
        "**Double click here to enter your answers to the questions in this text box.**\n",
        "\n",
        "<font color=\"green\" size=5>\n",
        "Answer:\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oiOqkET7ZVl_"
      },
      "outputs": [],
      "source": [
        "# Remove filtered images from metadata table\n",
        "meta = meta[~meta['filename'].isin(filtered_images)]\n",
        "# Reset the index for meta to be between 0 and 2438\n",
        "meta = meta.reset_index(drop=True)\n",
        "meta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tAFSgUknBJw"
      },
      "source": [
        "The final metadata table is shown above, after filtering out low quality images.\n",
        "\n",
        "Take a look at the table dimensions, shown at the bottom left.\n",
        "\n",
        "**CHALLENGE QUESTION:** How many images are we left with for training our classifiers?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tW8m1VeynTCZ"
      },
      "source": [
        "**Double click here to enter your answers to the questions in this text box.**\n",
        "<font color=\"green\" size=5>\n",
        "Answer:\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9vlUSwVkrEM"
      },
      "source": [
        "### Get Image Paths\n",
        "\n",
        "Finally, let's collect the paths to each image that we will use for training our classifiers. Although we already loaded in the data, this will be handy for accessing individual images later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ri1B4hjUNvLs"
      },
      "outputs": [],
      "source": [
        "# Get the filenames of all images in the bucket\n",
        "s3_client = boto3.client('s3', config=Config(signature_version=UNSIGNED))  # Creating a client for accessing S3 services\n",
        "\n",
        "paginator = s3_client.get_paginator('list_objects_v2')\n",
        "pages = paginator.paginate(Bucket=s3_bucket, Prefix=s3_dir_path)\n",
        "\n",
        "all_paths = []\n",
        "for page in pages:\n",
        "    for obj in page['Contents']:\n",
        "        all_paths.append(obj['Key'])\n",
        "\n",
        "len(all_paths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRtTon7mRGrz"
      },
      "outputs": [],
      "source": [
        "# Get paths to only the images in our subset metadata table\n",
        "\n",
        "paths_for_training = []\n",
        "for x in all_paths:\n",
        "  if x.split('/')[2] in list(meta['filename']):\n",
        "    paths_for_training.append(x)\n",
        "\n",
        "len(paths_for_training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzlt2BcnmahV"
      },
      "source": [
        "----\n",
        "\n",
        "# Exploratory Data Analysis (EDA)\n",
        "\n",
        "Exploratory Data Analysis (EDA) is a fundamental first step in using machine learning algorithms to analyze biological datasets. Before training any algorithm, we need to understand the structure of the data and any patterns, structures, or anomalies that would affect our analysis.\n",
        "\n",
        "In our case, we will be performing an EDA to understand how the different classes in our dataset (radiation exposure, dose, sex, strain) relate to each other. We will also be looking for outliers in the dataset, and for any imbalance in the classes (e.g. significantly more samples of one class than another).\n",
        "\n",
        "We will use techniques such as Principal Component Analysis (PCA), bar graphs, histograms, and image visualization techniques that we learned in the [Bioimaging Notebook](https://colab.research.google.com/drive/1n3PyMRgfrb4fWD53fgSuhtV364qUaW4r?usp=sharing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRYg6qHK8Tld"
      },
      "source": [
        "## Dimensionality Reduction Using PCA\n",
        "\n",
        "First, let's perform dimensionality reduction using PCA and then visualize the top two dimensions in our dataset using a scatter plot. This will allow us to visualize the top sources of variation in our dataset and look for patterns.\n",
        "\n",
        "We can create multiple PCA plots, colored by the different classes, to look for patterns within each class.\n",
        "\n",
        "This code has multiple steps:\n",
        "\n",
        "**1. Flatten the Images:**\n",
        "\n",
        "Typically, images are represented within code as 2D arrays (matrices) of pixel values. For example, a grayscale image of size 5x5 pixels is represented as a 5x5 matrix.\n",
        "<img src=\"https://drive.google.com/uc?id=1ylB588B0eYLGbHXXmgzFTKCdEjR8AL7x\">\n",
        "\n",
        "Flattening the images means transforming these multi-dimensional arrays into 1D vectors. For instance, a 5x5 grayscale image will be converted into a vector of length 25 (since 5*5=25).\n",
        "\n",
        "Algorithms like PCA typically expect the input data to be in a 2D format, where each row represents an observation (in this case, an image) and each column represents a feature (in this case, a pixel value).\n",
        "\n",
        "\n",
        "**2. Perform Dimensionality Reduction using t-SNE:**\n",
        "\n",
        "We then perform dimensionality reduction across the dataset using the [t-SNE algorithm](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html), which is similar to PCA. You can revisit PCA in the [Introduction to Clustering Lecture](https://www.youtube.com/watch?v=XPJmokNE7jY) and the [Clustering Notebook](https://colab.research.google.com/drive/1DdgTQUQ0hbNQxOfxK28kbA-z4Ilnof18?usp=sharing).\n",
        "\n",
        "t-SNE (t-Distributed Stochastic Neighbor Embedding) is a non-linear dimensionality reduction technique which we are using to visualize high-dimensional image data in 2 dimensions.\n",
        "\n",
        "First, we fit the t-SNE model to the data, which learns the internal structure. Then, we transform the data into the 2-dimensional space based on the learned structure. The output is a 2D array where each row corresponds to the same image in the input data but represented in the 2D space learned by t-SNE.\n",
        "\n",
        "**3. Plot the First Two Dimensions:**\n",
        "\n",
        "Finally, we plot the reduced dimensionality data in a scatter plot and color each point by the different groups within classes, to look for patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogt1DWCHHmVu"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFE0Xj03wmUc"
      },
      "outputs": [],
      "source": [
        "# Flatten the images\n",
        "X = resized_images.reshape(resized_images.shape[0], -1)\n",
        "\n",
        "# Perform dimensionality reduction using t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# Plot clusters color-coded by different labels\n",
        "plt.figure(figsize=(12, 12))\n",
        "\n",
        "# Plot and color by particle type\n",
        "plt.subplot(2, 2, 1)\n",
        "for label in meta['particle_type'].unique():\n",
        "    indices = meta[meta['particle_type'] == label].index # capture indices of images belonging to each class\n",
        "    plt.scatter(X_tsne[indices, 0], X_tsne[indices, 1], label=label, alpha=0.5) # match each index to its corresponding row in the reduced dim TSNE\n",
        "plt.title('Particle Type')\n",
        "plt.legend()\n",
        "\n",
        "# Plot and color by dose\n",
        "plt.subplot(2, 2, 2)\n",
        "for label in meta['dose_Gy'].unique():\n",
        "    indices = meta[meta['dose_Gy'] == label].index\n",
        "    plt.scatter(X_tsne[indices, 0], X_tsne[indices, 1], label=label, alpha=0.5)\n",
        "plt.title('Dose (Gy)')\n",
        "plt.legend()\n",
        "\n",
        "\n",
        "# Plot and color by sex\n",
        "plt.subplot(2, 2, 3)\n",
        "for label in meta['sex'].unique():\n",
        "    indices = meta[meta['sex'] == label].index\n",
        "    plt.scatter(X_tsne[indices, 0], X_tsne[indices, 1], label=label, alpha=0.5)\n",
        "plt.title('Sex')\n",
        "plt.legend()\n",
        "\n",
        "# Plot and color by sex\n",
        "plt.subplot(2, 2, 4)\n",
        "for label in meta['strain'].unique():\n",
        "    indices = meta[meta['strain'] == label].index\n",
        "    plt.scatter(X_tsne[indices, 0], X_tsne[indices, 1], label=label, alpha=0.5)\n",
        "plt.title('Strain')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhA_sWY0GDEi"
      },
      "source": [
        "**CHALLENGE QUESTIONS:**\n",
        "\n",
        "1. Take a look at the PCA plots above. Which class separates the dataset into clusters the best?\n",
        "2. Which class(es) do NOT separate the dataset, where the different groups are fully mixed throughout the dataset?\n",
        "3. For the class that separates the dataset the best, is it a full separation? Or is there some overlap between classes? If there is overlap, where in the cluster plot are the samples overlapping the most? What does this tell you biologically about the data?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I60JxfpLGcDt"
      },
      "source": [
        "**Double click here to enter your answers to the questions in this text box.**\n",
        "\n",
        "<font color=\"green\" size=5>\n",
        "Answers:\n",
        "<br>\n",
        "<ol>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "</ol>\n",
        "</font"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uXzLJDgLAq_"
      },
      "source": [
        "## Breakdown of Classes\n",
        "\n",
        "For training a machine learning algorithm, especially a binary classifier, it is important that the classes be approximately balanced (50% of the data in each class).\n",
        "\n",
        "Let's create some bar graphs to examine how balanced the dataset is in terms of radiation particle type exposure, mouse sex, and mouse strain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3IIOQxasgzu"
      },
      "outputs": [],
      "source": [
        "# Group by particle_type and sex, and count the occurrences\n",
        "grouped = meta.groupby(['particle_type', 'sex']).size().unstack()\n",
        "\n",
        "# Plotting\n",
        "ax = grouped.plot(kind='bar', stacked=True, figsize=(7, 4))\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.title('Distribution of Sex for Each Particle Type')\n",
        "plt.xlabel('Particle Type')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "# Place the legend outside the plot\n",
        "ax.legend(title='Sex', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T21_tg4JAW6P"
      },
      "outputs": [],
      "source": [
        "# Group by particle_type and sex, and count the occurrences\n",
        "grouped = meta.groupby(['particle_type', 'strain']).size().unstack()\n",
        "\n",
        "# Plotting\n",
        "ax = grouped.plot(kind='bar', stacked=True, figsize=(7, 4))\n",
        "\n",
        "# Adding titles and labels\n",
        "plt.title('Distribution of Strain for Each Particle Type')\n",
        "plt.xlabel('Particle Type')\n",
        "plt.ylabel('Count')\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "# Place the legend outside the plot\n",
        "ax.legend(title='Strain', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "# Show plot\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcunleTNKqyC"
      },
      "source": [
        "**CHALLENGE QUESTIONS:**\n",
        "\n",
        "Taking a look at the bar plots above, how balanced is the dataset for the following classes?\n",
        "\n",
        "1. Particle type\n",
        "2. Sex\n",
        "3. Strain\n",
        "4. Do you have any data balance concerns about training a binary classifier on this data, to classify each of these classes?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXVmFaJvKqyC"
      },
      "source": [
        "**Double click here to enter your answers to the questions in this text box.**\n",
        "\n",
        "<font color=\"green\" size=5>\n",
        "Answers:\n",
        "<br>\n",
        "<ol>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "</ol>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZSb83ZD8YfN"
      },
      "source": [
        "## Image Size Distributions\n",
        "\n",
        "To train a neural network on an imaging dataset, we need to ensure that our images are all the same size. As we discussed in the Bioimaging Notebook, this can be problematic if the images are very different sizes, since it can introduce artifacts into the data.\n",
        "\n",
        "Let's look at the distributions of image sizes (width and height) across classes to see whether resizing is appropriate. We can plot image width across the dataset as a histogram, to see if most of the images are around the same width for both the Fe and X-ray groups. We can do the same thing for the image height.\n",
        "\n",
        "\n",
        "First, we need to collect the width and height for all images in both groups.\n",
        "> **The next cell takes a minute or so to run.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzb83ezn0gZi"
      },
      "outputs": [],
      "source": [
        "# Collect width and height for the Fe and X-ray particle type classes\n",
        "xray=[]\n",
        "fe=[]\n",
        "for im, path in zip(loaded_images, paths_for_training):\n",
        "  w, h = im.size\n",
        "  particle = meta.set_index('filename').to_dict()['particle_type'][path.split('/')[2]]\n",
        "  if particle == 'X-ray':\n",
        "    xray.append((w,h))\n",
        "  if particle == 'Fe':\n",
        "    fe.append((w,h))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5iRVQJBL6TN"
      },
      "source": [
        "Next, let's plot a histogram of ***image width*** across our dataset, split into the X-ray and Fe groups from the particle type class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhG47ZJ8B4Qa"
      },
      "outputs": [],
      "source": [
        "plt.hist([x[0] for x in xray], label='X-ray', alpha=0.5)\n",
        "plt.hist([x[0] for x in fe], label='Fe', alpha=0.5)\n",
        "plt.ylabel('Number of images')\n",
        "plt.xlabel('Image Width (pixels)')\n",
        "plt.legend()\n",
        "plt.title('Width')\n",
        "\n",
        "# Get mean width for X-ray and Fe\n",
        "print('Mean Width X-ray Images: ' + str(np.mean([x[0] for x in xray])))\n",
        "print('Mean Width Fe Images: ' + str(np.mean([x[0] for x in fe])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "liQ9TnMXMM6y"
      },
      "source": [
        "Next, let's plot a histogram of ***image height*** across our dataset, split into the X-ray and Fe groups from the particle type class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0g6RFqhIK_5"
      },
      "outputs": [],
      "source": [
        "plt.hist([x[1] for x in xray], label='X-ray', alpha=0.5)\n",
        "plt.hist([x[1] for x in fe], label='Fe', alpha=0.5)\n",
        "plt.ylabel('Number of images')\n",
        "plt.xlabel('Image Height (pixels)')\n",
        "plt.legend()\n",
        "plt.title('Height')\n",
        "\n",
        "# Get mean height for X-ray and Fe\n",
        "print('Mean Height X-ray Images: ' + str(np.mean([x[1] for x in xray])))\n",
        "print('Mean Height Fe Images: ' + str(np.mean([x[1] for x in fe])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_YJm-YsMVXb"
      },
      "source": [
        "**CHALLENGE QUESTIONS:**\n",
        "\n",
        "1. Taking a look at the histograms above, do the X-ray and Fe images appear to follow a similar distribution in terms of width? Height?\n",
        "2. Are the width and height of the images in both groups spread out, or do the majority of the images seem to have a similar width or height?\n",
        "3. What is the approximate value of the width of the majority of the images? Height? (*Hint:* Take a look at the mean values printed out before each plot.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CIpQTTIKMVXb"
      },
      "source": [
        "**Double click here to enter your answers to the questions in this text box.**\n",
        "\n",
        "<font color=\"green\" size=5>\n",
        "Answers:\n",
        "<br>\n",
        "<ol>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "</ol>\n",
        "</font"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3472f-pbkHJi"
      },
      "source": [
        "----\n",
        "\n",
        "> **NOTE:** Based on the results of the image size distributions, **we will proceed with resizing our images for neural networks to a consistent square size of 140x140 pixels**. This is slightly larger than the mean for both height and width, since the distribution is slightly skewed toward the larger side (the right side of each plot). Because both of our main groups (X-ray and Fe exposure) have very similar distributions, we do not expect resizing to introduce any artifacts in one group that are not also present in the other, so we will not be impacting the performance of our neural networks.\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jNHBr6L_MpHA"
      },
      "source": [
        "## Selected Image Visualizations\n",
        "\n",
        "Let's take a look at a few of the images in our dataset.\n",
        "\n",
        "Recall that the X-ray exposure causes a diffuse or \"cloudy\" DNA damage signal, which sometimes includes distinct spots of DNA damage without a specific pattern.\n",
        "\n",
        "Recall also that the Fe heavy ion exposure can leave a characteristic linear \"track\" of DNA damage through the cell, but if the Fe ion did not directly impact the cell, there may not be any track. In this case, the Fe exposure can appear more like the X-ray exposure.\n",
        "\n",
        "We'll image 10 random Xray-exposed samples, and 10 random Fe-exposed samples.\n",
        "\n",
        ">*You can rerun these cells many times to get a different selection of 10 random images.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM2RC_mJioN0"
      },
      "source": [
        "----\n",
        "\n",
        "**10 Random Xray-exposed Images:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmy3_cEShZpn"
      },
      "outputs": [],
      "source": [
        "# Select 10 random X-ray images\n",
        "random_xray_indices = np.random.choice(meta[meta['particle_type']=='X-ray'].index, 10, replace=False)\n",
        "\n",
        "# Create a figure for the grid\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Plot each misclassified image\n",
        "for ax, idx in zip(axes, random_xray_indices):\n",
        "  img = load_image_from_s3(idx, meta)\n",
        "  ax.imshow(img, vmin=400, vmax=4000)\n",
        "\n",
        "fig.suptitle('10 Random Xray-exposed Images', fontsize=16)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUdGl5Mkiepe"
      },
      "source": [
        "---\n",
        "**10 Random Fe-exposed Images:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icZri_R9gKn7"
      },
      "outputs": [],
      "source": [
        "# Select 10 random Fe images\n",
        "random_Fe_indices = np.random.choice(meta[meta['particle_type']=='Fe'].index, 10, replace=False)\n",
        "\n",
        "# Create a figure for the grid\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Plot each misclassified image\n",
        "for ax, idx in zip(axes, random_Fe_indices):\n",
        "  img = load_image_from_s3(idx, meta)\n",
        "  ax.imshow(img, vmin=400, vmax=4000)\n",
        "\n",
        "fig.suptitle('10 Random Fe-exposed Images', fontsize=16)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkdjgHjji6uy"
      },
      "source": [
        "**CHALLENGE QUESTIONS:**\n",
        "\n",
        "1. Re-run the X-ray cell a few times. What patterns show up consistently? Does this make sense given how X-rays cause DNA damage patterns within cells?\n",
        "\n",
        "2. Re-run the Fe cell a few times. How often does a cell with a distinct linear track come up? What other patterns do you see?\n",
        "\n",
        "3. Based on the differences and similarities between X-ray and Fe exposed cells, do you expect it to be easy or difficult for a machine learning classifier to learn the differences between the two types of radiation exposure?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "464eaJvcjZ-E"
      },
      "source": [
        "**Double click here to enter your answers to the questions in this text box.**\n",
        "\n",
        "<font color=\"green\" size=5>\n",
        "Answers:\n",
        "<br>\n",
        "<ol>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "</ol>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ikgwgq6CfxKL"
      },
      "source": [
        "----\n",
        "\n",
        "# Radiation Type Classifiers\n",
        "\n",
        "Now that we've completed our EDA, let's focus on the questions we outlined at the beginning of this Capstone Project.\n",
        "\n",
        "- **Can we train a machine learning classifier to classify radiation type (X-ray vs. Fe)?**\n",
        "  - **What type of classifier works best for this task? Why?**\n",
        "- **What do the misclassified images have in common? What about the correctly classified images?**\n",
        "  - **What does this tell us about what biological patterns are being used by the models?**\n",
        "\n",
        "To answer these questions, we'll explore the use of two powerful and widely-used machine learning classifiers: Support Vector Machines (SVM) and Convolutional Neural Networks (CNN). These classifiers have proven effective in various classification tasks, but they have distinct characteristics and strengths that make them suitable for different types of data and problems.\n",
        "\n",
        "### Support Vector Machines (SVM)\n",
        "\n",
        "SVMs are a type of supervised learning algorithm that can be used for both classification and regression challenges. The key idea behind SVMs is to find the optimal hyperplane that separates the data points of different classes with the maximum margin. Here are some of the main features of SVMs:\n",
        "\n",
        "- **High-dimensional space handling:** SVMs perform well in high-dimensional spaces and are effective when the number of dimensions exceeds the number of samples.\n",
        "- **Effective with small datasets:** SVMs are particularly useful when the dataset is small but well-defined.\n",
        "- **Kernel trick:** SVMs can handle non-linear data through the use of kernel functions, which transform the data into a higher-dimensional space where a linear separator can be found.\n",
        "\n",
        "In our context, we will train an SVM classifier to distinguish between X-ray and Fe radiation types using features extracted from the dataset. We'll evaluate the performance of the SVM using metrics such as accuracy, precision, recall, and F1-score. We will also take a look at the support vector plot, and the incorrectly classified images, to understand the SVM model's strengths and weaknesses.\n",
        "\n",
        "### Convolutional Neural Networks (CNN)\n",
        "\n",
        "CNNs are a class of deep learning models specifically designed to process and analyze image data. They are highly effective in recognizing patterns and structures in images due to their convolutional layers, which automatically learn spatial hierarchies of features. Here are some key characteristics of CNNs:\n",
        "\n",
        "- **Automatic feature extraction:** CNNs automatically learn relevant features from raw image data, eliminating the need for manual feature extraction.\n",
        "- **Translation invariance:** The convolutional layers make CNNs robust to shifts and translations in the input images.\n",
        "- **Scalability:** CNNs can scale to large datasets and complex tasks, often outperforming traditional machine learning models in image classification tasks.\n",
        "\n",
        "For our task, we will train a CNN to classify images of radiation types (X-ray vs. Fe). The CNN will learn from raw image data and extract features that help in distinguishing between the two types. We will assess the CNN's performance using the same set of evaluation metrics as for the SVM.\n",
        "\n",
        "### Comparison and Evaluation\n",
        "\n",
        "To determine which classifier works best for classifying radiation types, we will compare the performance of the SVM and CNN models based on the following criteria:\n",
        "\n",
        "- **Accuracy:** The proportion of correctly classified samples out of the total samples.\n",
        "- **Precision:** The ratio of true positive predictions to the total positive predictions, indicating the accuracy of positive predictions.\n",
        "- **Recall:** The ratio of true positive predictions to the total actual positives, indicating the ability to identify all positive samples.\n",
        "- **F1-score:** The harmonic mean of precision and recall, providing a balance between the two metrics.\n",
        "\n",
        "By training both models and evaluating their performance on a test set, we will gain insights into the strengths and weaknesses of SVMs and CNNs for this specific classification task. This comparison will help us identify the most suitable classifier for distinguishing between X-ray and Fe radiation types and understand the underlying reasons for its effectiveness.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRIchI4_gfeA"
      },
      "source": [
        "First, let's isolate only the \"particle_type\" column from the metadata table. This is the column with information on which radiation type the cell in each image was exposed to (Fe or X-ray)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4ghvSgbUnGm"
      },
      "outputs": [],
      "source": [
        "# Get the particle labels only\n",
        "particle_labels = meta[['filename','particle_type']]\n",
        "particle_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpDNzOVHZYwE"
      },
      "source": [
        "## SVM\n",
        "\n",
        "Next, let's train the SVM. We're using the SVC implementation of SVM in scikit-learn (sklearn). You can read more about this implementation [here](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html).\n",
        "\n",
        "**Steps to train the SVM:**\n",
        "- Encoding labels: We encode the class labels (\"Fe\" and \"X-ray\") as numerical values (e.g. 0 and 1), since the SVM only takes numerical data.\n",
        "- Split data into 80% training and 20% testing\n",
        "- Flatten image arrays: flatten each 2D array into a 1D vector for input to the SVM\n",
        "- Train the SVM model using a Radial Basis Function (RBF) kernel. Because it relies on a Gaussian function, the RBF kernel is often the best for image classification tasks.\n",
        "- Perform predictions using the held-out 20% training data in order to calculate the accuracy of the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZRqWcE8j3V05"
      },
      "outputs": [],
      "source": [
        "# Import libraries for SVM and downstream analysis\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl0EZkB4ZZ3j"
      },
      "outputs": [],
      "source": [
        "# Encoding labels\n",
        "label_encoder = LabelEncoder()\n",
        "particle_labels['particle_type_encoded'] = label_encoder.fit_transform(particle_labels['particle_type'])\n",
        "\n",
        "# Set up X and y variables for data and labels\n",
        "X = resized_images\n",
        "y = particle_labels['particle_type_encoded'].values\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Flatten image arrays\n",
        "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "# Train SVM model\n",
        "svm_model = SVC(kernel='rbf')\n",
        "svm_model.fit(X_train_flat, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = svm_model.predict(X_test_flat)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJmhHN-GKgzO"
      },
      "source": [
        "> Take a look at the classification report from the SVM printed above.\n",
        "\n",
        "**CHALLENGE QUESTIONS:**\n",
        "\n",
        "1. What is the overall accuracy of the SVM model?\n",
        "2. How many images from each class were used in the test data to calculate the accuracy and other metrics? (*Hint:* Take a look at the \"support\" column.) Are the classes approximately equally split?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duo71wItNXh2"
      },
      "source": [
        "**Double click here to enter your answers to the questions in this text box.**\n",
        "\n",
        "<font color=\"green\" size=5>\n",
        "Answers:\n",
        "<br>\n",
        "<ol>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "</ol>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EOCloGjvLlNo"
      },
      "source": [
        "#### Confusion Matrix\n",
        "\n",
        "Now, let's generate the confusion matrix as another way to visualize the correctly and incorrectly classified images.\n",
        "\n",
        "As a reminder, a confusion matrix summarizes the performance of a classification model by comparing its predicted class labels against the true class labels, to show how many samples are correctly and incorrectly classified.\n",
        "\n",
        "A confusion matrix consists of a square grid with rows representing the actual (true) class labels and and columns representing predicted class labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jLzuJtf8d9j"
      },
      "outputs": [],
      "source": [
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k67P8XmfLMi2"
      },
      "source": [
        "**CHALLENGE QUESTION:**\n",
        "\n",
        "1. Is there an imbalance in the class predictions? For example, is one class consistently being wrongly predicted as the other? Or is it fairly balanced?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCE6HqvMNaBG"
      },
      "source": [
        "**Double click here to enter your answers to the questions in this text box.**\n",
        "\n",
        "<font color=\"green\" size=5>\n",
        "Answer:\n",
        "<br>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkm-qZ3ELnUs"
      },
      "source": [
        "#### Support Vector Plot\n",
        "\n",
        "Let's generate a Support Vector Plot to examine where the support vectors fall in the overall data distribution.\n",
        "\n",
        "As a reminder, the support vectors are the data points that are closest to the decision boundary or hyperplane. They are the most critical points that determine where the decision boundary lies. NOTE: on this plot, the hyperplane is not visible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d097qPaOyIx0"
      },
      "outputs": [],
      "source": [
        "# Get support vectors\n",
        "support_vectors = svm_model.support_vectors_\n",
        "\n",
        "# Plot support vectors\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(X_train_flat[:, 0], X_train_flat[:, 1], c=y_train, cmap=plt.cm.Paired, marker='o', label='Training Data')\n",
        "plt.scatter(support_vectors[:, 0], support_vectors[:, 1], s=100, facecolors='none', edgecolors='k', label='Support Vectors')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.title('Support Vectors')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIw8H_PJLuHN"
      },
      "source": [
        "Take a look at the support vector plot above. It may be difficult to see all of the data points, so focus on the middle of the graph where there are fewer points to answer the next few questions.\n",
        "\n",
        "**CHALLENGE QUESTIONS:**\n",
        "\n",
        "1. Are there a lot of support vectors, or only a few?\n",
        "2. Imagine trying to draw a line (the hyperplane) that separates all of the support vectors from different classes. Would this be a difficult or easy task?\n",
        "3. Based on your answer to #2, do you think this was an easy or difficult classification task for the SVM?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFFtjVJvNc7c"
      },
      "source": [
        "**Double click here to enter your answers to the questions in this text box.**\n",
        "\n",
        "<font color=\"green\" size=5>\n",
        "Answers:\n",
        "<br>\n",
        "<ol>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "</ol>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW8A1Q_QLrUc"
      },
      "source": [
        "#### Misclassified Images Analysis\n",
        "\n",
        "To get a better sense of which images within the dataset are being misclassified, we can return to our radiation type PCA plot from the EDA, and label the points on that plot that correspond to images that the SVM misclassified. This will give us further insight into the relationships between the misclassified images.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P95ezofZv6zj"
      },
      "outputs": [],
      "source": [
        "# Flatten the images\n",
        "X = resized_images.reshape(resized_images.shape[0], -1)\n",
        "\n",
        "# Perform dimensionality reduction using PCA\n",
        "pca = PCA(n_components=50)  # Adjust the number of components as needed\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Perform dimensionality reduction using t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# Obtain the original indices of the test set\n",
        "_, X_test_indices = train_test_split(range(len(resized_images)), test_size=0.2, random_state=42)\n",
        "\n",
        "# Identify misclassified points\n",
        "misclassified_indices = np.where(y_pred != y_test)[0]\n",
        "\n",
        "# Extract the original indices of the misclassified test samples\n",
        "misclassified_tsne_indices = [X_test_indices[i] for i in misclassified_indices]\n",
        "\n",
        "# Plot clusters color-coded by different labels\n",
        "plt.figure(figsize=(7,7))\n",
        "\n",
        "# Plot and color by particle type\n",
        "for label in meta['particle_type'].unique():\n",
        "    indices = meta[meta['particle_type'] == label].index  # capture indices of images belonging to each class\n",
        "    plt.scatter(X_tsne[indices, 0], X_tsne[indices, 1], label=label, alpha=0.3)  # match each index to its corresponding row in the reduced dim TSNE\n",
        "\n",
        "# Highlight misclassified points\n",
        "plt.scatter(X_tsne[misclassified_tsne_indices, 0], X_tsne[misclassified_tsne_indices, 1], c='red', marker='x', label='Misclassified', alpha=1)\n",
        "\n",
        "plt.title('Particle Type with Misclassified Points Highlighted')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVVSOgdPMeNM"
      },
      "source": [
        "Take a look at the PCA plot above, with the SVM misclassified images labeled in red \"x\", to answer the following questions.\n",
        "\n",
        "**CHALLENGE QUESTIONS:**\n",
        "\n",
        "1. Are the misclassified images evenly distributed throughout the PCA plot, or are they clustering in one specific area?\n",
        "2. What does your answer to #2 tell you about which types of images were most difficult for the SVM to correctly classify? How do you interpret this biologically?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5ahdY7yNfMs"
      },
      "source": [
        "**Double click here to enter your answers to the questions in this text box.**\n",
        "\n",
        "<font color=\"green\" size=5>\n",
        "Answers:\n",
        "<br>\n",
        "<ol>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "</ol>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoZwZCPFM31D"
      },
      "source": [
        "----\n",
        "\n",
        "Let's take a look at 10 random misclassified images from the SVM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xv_r0DJkLKCT"
      },
      "outputs": [],
      "source": [
        "# Select 10 random misclassified indices\n",
        "random_misclassified_tsne_indices = np.random.choice(misclassified_tsne_indices, 10, replace=False)\n",
        "\n",
        "# Create a figure for the grid\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Plot each misclassified image\n",
        "for ax, idx in zip(axes, random_misclassified_tsne_indices):\n",
        "  img = load_image_from_s3(idx, meta)\n",
        "  ax.imshow(img, vmin=400, vmax=4000)\n",
        "  ax.axis('off')\n",
        "  true_label = particle_labels['particle_type_encoded'][idx]\n",
        "  true_display = 'Fe' if true_label == 0 else 'X-ray'\n",
        "  predicted_label = 0 if true_label == 1 else 1\n",
        "  predicted_display = 'Fe' if predicted_label == 0 else 'X-ray'\n",
        "  ax.set_title(f'True: {true_display}, Pred: {predicted_display}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gjJZvHZYNvkX"
      },
      "source": [
        "Take a look at the misclassified images above. You can re-run the code cell above a few times to get a sense of the patterns.\n",
        "\n",
        "**CHALLENGE QUESTIONS:**\n",
        "\n",
        "1. What trends do you observe in the Fe-exposed images that are being misclassified as X-ray?\n",
        "\n",
        "2. What trends do you observe in the Xray-exposed images that are being misclassified as Fe?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dofZg_w6ODF4"
      },
      "source": [
        "**Double click here to enter your answers to the questions in this text box.**\n",
        "\n",
        "<font color=\"green\" size=5>\n",
        "Answers:\n",
        "<br>\n",
        "<ol>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "</ol>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-01vUyFOEXs"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4vWW5ASZg4n"
      },
      "source": [
        "## CNN\n",
        "\n",
        "Next, let's train the CNN.\n",
        "\n",
        "> **NOTE:** The cell below takes at least 30 minutes to run because it trains the CNN for 10 epochs. Time for a coffee break! Just make sure that your computer stays awake if you step away.\n",
        "\n",
        "**Steps that we use for training the CNN below:**\n",
        "\n",
        "- *Split the dataset into 80% training and 20% validation data.* We refer to the held-out data as validation in this case because it is often used for validating the hyperparameters and architecture of the model. A third step that we will not perform here is to hold-out an additional 20% as a testing set for the final accuracy calculation.\n",
        "- *Define and use data generators for the training and validation datasets.* This function pulls data from the S3 bucket, resizes each image, flattens each array, and organizes the data and labels in a format that the CNN can read.\n",
        "- *Define model parameters.*\n",
        "  - We use **batch size = 32** since this will give us approximately 60 steps per epoch. If the batch size is much smaller than this, an epoch will take a long time, and if it is much larger, the model will lose detail. (Recall that in each epoch, the model iterates through the training data once, and each step in an epoch pulls in data the size of a batch).\n",
        "  - We use **image input shape** = 140x140 pixels as determined in our EDA. The third value represents the channel, and \"1\" indicates that each pixel in our images is represented by a single intensity value.\n",
        "  - We set **number of classes = 2** since we have 2 classes (Fe and X-ray), and we set **number of epochs = 10** so that our model does not train for too long.\n",
        "  - Finally, **steps per training epoch** and **steps per validation epoch** are simply calculations relying on the number of images in the training and validation sets, divided by the batch size.\n",
        "\n",
        "- *Build the model.* We are using a simple CNN architecture as outlined below.\n",
        "\n",
        "  1. **First Convolutional Layer:** This layer looks at small 3x3 patches of the input image and creates 32 different versions of these patches, each emphasizing different features (like edges or textures). It then applies the ReLU function, which helps the model to understand more complex patterns by turning all negative values to zero.\n",
        "    - In simpler terms, ReLU \"turns off\" negative values and lets positive values pass through unchanged. This helps the neural network activate (or \"fire\") only when it detects important features, making the learning process more efficient and effective.\n",
        "\n",
        "  2. **First Pooling Layer:** This layer reduces the size of the image by half. It does this by picking the highest value from each 2x2 block in the previous layer, which helps to focus on the most important features and makes the computation easier.\n",
        "\n",
        "  3. **Second Convolutional Layer:** Similar to the first one, but now it uses 64 filters to capture even more detailed features from the already processed image.\n",
        "\n",
        "  4. **Second Pooling Layer:** Again, this reduces the size by half using the same method as the first pooling layer, further summarizing the features detected.\n",
        "\n",
        "  5. **Third Convolutional Layer:** This layer uses 128 filters, allowing the model to capture even finer details of the image.\n",
        "\n",
        "  6. **Third Pooling Layer:** This layer performs another size reduction, focusing on the most significant features detected so far.\n",
        "\n",
        "  7. **Flatten Layer:** This layer takes the pooled feature maps (which are still in a 2D grid form) and flattens them into a 1D vector. This transformation allows the next layer to process the data.\n",
        "\n",
        "  8. **First Dense (Fully Connected) Layer:** This layer has 128 neurons and processes the feature map vector, learning complex patterns. It also uses the ReLU function to help the model understand complex patterns by setting all negative numbers to zero.\n",
        "\n",
        "  9. **Output Layer:** This layer has 2 neurons - 1 for each class. It uses the softmax function to turn the numbers into probabilities that add up to 100%, indicating the likelihood that the image belongs to each class. (For example, an image could be given a 30% likelihood that it belongs to the Fe class, and a 70% likelihood that it belongs to the X-ray class).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MwHo31LQOOUq"
      },
      "outputs": [],
      "source": [
        "# Import libraries for CNN training and downstream analysis\n",
        "import os\n",
        "import boto3\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import tensorflow as tf\n",
        "from botocore import UNSIGNED\n",
        "from botocore.client import Config\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dDbRkcW_40w"
      },
      "outputs": [],
      "source": [
        "# Split the dataset 80-20 training - validation\n",
        "train_labels = particle_labels.sample(int(0.8*len(particle_labels))) # get random 80% images and their labels\n",
        "train_paths = ['Microscopy/train/' + x for x  in list(train_labels['filename'])] # get the filepaths for those images in the same order\n",
        "\n",
        "val_labels = particle_labels[~particle_labels.isin(train_labels)].dropna() # get the other 20% of the images and their labels\n",
        "val_paths = ['Microscopy/train/' + x for x  in list(val_labels['filename'])] # and get their filepaths\n",
        "\n",
        "# Define model parameters\n",
        "batch_size = 32\n",
        "input_shape = (140, 140, 1)\n",
        "num_classes = 2\n",
        "epochs = 10\n",
        "steps_per_epoch = len(train_paths) // batch_size\n",
        "validation_steps = len(val_paths) // batch_size\n",
        "\n",
        "# Define data generators for training and validation\n",
        "def generate_batches_from_s3(object_keys, labels_df, batch_size, input_shape):\n",
        "    indexes = np.arange(len(labels_df))\n",
        "    while True:\n",
        "        for i in range(0, len(labels_df), batch_size):\n",
        "            num_rows = len(labels_df)\n",
        "            batch_indexes = indexes[i:i+batch_size]\n",
        "            X = np.empty((len(batch_indexes), *input_shape))\n",
        "            y = np.empty((len(batch_indexes)), dtype=int)\n",
        "            for j, idx in enumerate(batch_indexes):\n",
        "                row = labels_df.iloc[idx]\n",
        "                img_filename = row['filename']\n",
        "                img_label = 0 if row['particle_type'] == 'Fe' else 1\n",
        "                obj = s3_client.get_object(Bucket=s3_bucket, Key=os.path.join(s3_dir_path, img_filename))\n",
        "                img_bytes = obj['Body'].read()\n",
        "                img = Image.open(BytesIO(img_bytes))\n",
        "                img = img.resize((input_shape[0], input_shape[1]))\n",
        "                img = np.array(img)\n",
        "                img = np.clip(img, 400, 4000) # mimicking imshow vmin/vmax https://stackoverflow.com/questions/31232733/vmin-vmax-algorithm-matplotlib\n",
        "                X[j] = np.expand_dims(img, axis=-1)\n",
        "                y[j] = img_label\n",
        "            yield X, y\n",
        "\n",
        "\n",
        "# Create data generators for training and validation\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: generate_batches_from_s3(train_paths, train_labels, batch_size, input_shape),\n",
        "    output_types=(tf.float32, tf.int32),\n",
        "    output_shapes=(tf.TensorShape([None, *input_shape]), tf.TensorShape([None])),\n",
        ").repeat()\n",
        "\n",
        "validation_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: generate_batches_from_s3(val_paths, val_labels, batch_size, input_shape),\n",
        "    output_types=(tf.float32, tf.int32),\n",
        "    output_shapes=(tf.TensorShape([None, *input_shape]), tf.TensorShape([None])),\n",
        ").repeat()\n",
        "\n",
        "# Build CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_history = model.fit(train_dataset, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=validation_dataset, validation_steps=validation_steps)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fp690PHu_bA8"
      },
      "source": [
        "**CHALLENGE QUESTIONS:**\n",
        "\n",
        "1. What was the final training accuracy?\n",
        "\n",
        "2. What was the final validation accuracy?\n",
        "\n",
        "3. Based on accuracy alone, which model performed better at our classification task, SVM or CNN?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4ct3NUo_sjP"
      },
      "source": [
        "**Double click here to enter your answers to the questions in this text box.**\n",
        "\n",
        "<font color=\"green\" size=5>\n",
        "Answers:\n",
        "<br>\n",
        "<ol>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "</ol>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvBQbPMoKneg"
      },
      "source": [
        "#### Model Accuracy Plot\n",
        "\n",
        "Let's create a model accuracy plot to see how the training and validation accuracy changed over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MoggPgymSjL9"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_accuracy = model_history.history['accuracy']\n",
        "val_accuracy = model_history.history['val_accuracy']\n",
        "epochs = range(1, len(train_accuracy) + 1)\n",
        "\n",
        "plt.plot(epochs, train_accuracy, 'b', label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'r', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8_mm3ds_4TH"
      },
      "source": [
        "**CHALLENGE QUESTIONS:**\n",
        "\n",
        "1. Using the accuracy plot above, do you see any signs of overfitting? What about underfitting?\n",
        "\n",
        "2. Do you think that we should train the model for more epochs, or do you think that we have reached the best possible accuracy for this model and this subset of the dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aozKJKcsALbv"
      },
      "source": [
        "**Double click here to enter your answers to the questions in this text box.**\n",
        "\n",
        "<font color=\"green\" size=5>\n",
        "Answers:\n",
        "<br>\n",
        "<ol>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "</ol>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7_8FiTtKx2C"
      },
      "source": [
        "#### Misclassified Images Analysis\n",
        "\n",
        "Just like we did after training the SVM, we can return to our radiation type PCA plot from the EDA, and label the points on that plot that correspond to images that the SVM misclassified. This will give us further insight into the relationships between the misclassified images.\n",
        "\n",
        "**NOTE: the cell below takes a few minutes to run.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxZW7XT0no8w"
      },
      "outputs": [],
      "source": [
        "# Flatten the images\n",
        "X = resized_images.reshape(resized_images.shape[0], -1)\n",
        "\n",
        "# Perform dimensionality reduction using PCA\n",
        "pca = PCA(n_components=50)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "# Perform dimensionality reduction using t-SNE\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_tsne = tsne.fit_transform(X)\n",
        "\n",
        "# Initialize arrays to store prediction results\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "\n",
        "# Iterate over the validation dataset and collect predictions\n",
        "for i, (X_batch, y_batch) in enumerate(validation_dataset.take(validation_steps)):\n",
        "    batch_predictions = model.predict(X_batch)\n",
        "    batch_predicted_labels = np.argmax(batch_predictions, axis=1)\n",
        "    predicted_labels.extend(batch_predicted_labels)\n",
        "    true_labels.extend(y_batch.numpy())\n",
        "\n",
        "    #print(f\"Step {i+1}/{validation_steps} completed.\")\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "predicted_labels = np.array(predicted_labels)\n",
        "true_labels = np.array(true_labels)\n",
        "\n",
        "# Identify misclassified indices\n",
        "misclassified_indices = np.where(predicted_labels != true_labels)[0]\n",
        "\n",
        "\n",
        "# Print the number of misclassified images\n",
        "num_misclassified = len(misclassified_indices)\n",
        "print(f\"Number of misclassified images: {num_misclassified}\")\n",
        "\n",
        "\n",
        "# Plot clusters color-coded by different labels\n",
        "plt.figure(figsize=(7,7))\n",
        "\n",
        "# Plot and color by particle type\n",
        "\n",
        "for label in meta['particle_type'].unique():\n",
        "    indices = meta[meta['particle_type'] == label].index\n",
        "    plt.scatter(X_tsne[indices, 0], X_tsne[indices, 1], label=label, alpha=0.5)\n",
        "# Highlight misclassified points\n",
        "plt.scatter(X_tsne[misclassified_indices, 0], X_tsne[misclassified_indices, 1], color='red', marker='x', label='Misclassified', alpha=0.7)\n",
        "plt.title('Particle Type')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaDGYX43BIur"
      },
      "source": [
        "Take a look at the PCA plot above, with the CNN misclassified images labeled in red \"x\", to answer the following questions.\n",
        "\n",
        "**CHALLENGE QUESTIONS:**\n",
        "\n",
        "1. Are the misclassified images evenly distributed throughout the PCA plot, or are they clustering in one specific area?\n",
        "2. What does your answer to #2 tell you about which types of images were most difficult for the CNN to correctly classify? How do you interpret this biologically?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FZLucKMSBIu3"
      },
      "source": [
        "**Double click here to enter your answers to the questions in this text box.**\n",
        "\n",
        "<font color=\"green\" size=5>\n",
        "Answers:\n",
        "<br>\n",
        "<ol>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "</ol>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wMdcQ0GeBXpC"
      },
      "source": [
        "----\n",
        "\n",
        "Let's take a look at 10 random misclassified images *and* 10 random correctly classified images from the CNN.\n",
        "\n",
        "**10 Random Misclassified Images from CNN:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GAOQFdZPfa_"
      },
      "outputs": [],
      "source": [
        "# Select 10 random misclassified indices\n",
        "random_misclassified_indices = np.random.choice(misclassified_indices, 10, replace=False)\n",
        "\n",
        "# Create a figure for the grid\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Plot each misclassified image\n",
        "for ax, idx in zip(axes, random_misclassified_indices):\n",
        "  img = load_image_from_s3(idx, val_labels)\n",
        "  ax.imshow(img, vmin=400, vmax=4000)\n",
        "  ax.axis('off')\n",
        "  true_label = true_labels[idx]\n",
        "  true_display = 'Fe' if true_label == 0 else 'X-ray'\n",
        "  predicted_label = predicted_labels[idx]\n",
        "  predicted_display = 'Fe' if predicted_label == 0 else 'X-ray'\n",
        "  ax.set_title(f'True: {true_label}, Pred: {predicted_label}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ym3h_cD_CGM6"
      },
      "source": [
        "----\n",
        "\n",
        "**10 Random Correctly Classified Images from CNN:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VDLR4fwsAyAX"
      },
      "outputs": [],
      "source": [
        "correct_indices = np.where(predicted_labels == true_labels)[0]\n",
        "\n",
        "# Select 10 random correctly classified indices\n",
        "random_correct_indices = np.random.choice(correct_indices, 10, replace=False)\n",
        "\n",
        "# Create a figure for the grid\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
        "axes = axes.flatten()\n",
        "\n",
        "# Plot each correctly classified image\n",
        "for ax, idx in zip(axes, random_correct_indices):\n",
        "  img = load_image_from_s3(idx, val_labels)\n",
        "  ax.imshow(img, vmin=400, vmax=4000)\n",
        "  ax.axis('off')\n",
        "  true_label = true_labels[idx]\n",
        "  predicted_label = predicted_labels[idx]\n",
        "  ax.set_title(f'True: {true_label}, Pred: {predicted_label}')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLZcqtpACL2S"
      },
      "source": [
        "----\n",
        "\n",
        "You can rerun the two cells above to get a sense of which types of images are being misclassified.  \n",
        "\n",
        "**CHALLENGE QUESTIONS:**\n",
        "\n",
        "1. What patterns do you see consistently in the Fe images that are being misclassified as X-ray? What about the Fe images that are being *correctly* classified as X-ray?\n",
        "\n",
        "2. What patterns do you see consistently in the X-ray images that are being misclassified as Fe? What about the X-ray images that are being *correctly* classified as X-ray?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO8f8Fi5C5P7"
      },
      "source": [
        "**Double click here to enter your answers to the questions in this text box.**\n",
        "\n",
        "<font color=\"green\" size=5>\n",
        "Answers:\n",
        "<br>\n",
        "<ol>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "</ol>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UzKX2VRg1Ep"
      },
      "source": [
        "# Sex Classifier\n",
        "\n",
        "Now that we've successfully trained and evaluated two different classifiers to predict the *radiation exposure type* for each image, let's return to the other questions we outlined at the beginning of this project:\n",
        "\n",
        "- Can we train a machine learning classifier to classify the *sex* of the mouse from which each cell nucleus 53bp1+ DNA damage image was taken?\n",
        "  - What about the *strain* of the mouse?\n",
        "- If not, what does this tell us about what biological traits are being captured in the 53bp1+ DNA damage cell nucleus images?\n",
        "\n",
        "----\n",
        "\n",
        "Let's train the same SVM and CNN architectures but instead we will use the *sex* of the mouse that each cellular image came from as the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOs1NU-ZkR4y"
      },
      "outputs": [],
      "source": [
        "# Get the sex labels only\n",
        "sex_labels = meta[['filename','sex']]\n",
        "sex_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Prp-RojBoHJn"
      },
      "source": [
        "## SVM\n",
        "\n",
        "We will train an SVM on the sex labels below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6kkxsfVqoHJo"
      },
      "outputs": [],
      "source": [
        "# Import libraries for SVM and downstream analysis\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7H3lRMV8oHJo"
      },
      "outputs": [],
      "source": [
        "# Encoding labels\n",
        "label_encoder = LabelEncoder()\n",
        "sex_labels['sex_type_encoded'] = label_encoder.fit_transform(sex_labels['sex'])\n",
        "\n",
        "# Load and preprocess images\n",
        "X = resized_images\n",
        "y = sex_labels['sex_type_encoded'].values\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Flatten image arrays\n",
        "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "# Train SVM model\n",
        "svm_model = SVC(kernel='rbf')\n",
        "svm_model.fit(X_train_flat, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = svm_model.predict(X_test_flat)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ui7X5RTbu_1p"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Predictions\n",
        "y_pred = svm_model.predict(X_test_flat)\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yt6uE7ZSF4JQ"
      },
      "source": [
        "**CHALLENGE QUESTION:**\n",
        "\n",
        "1. How did the SVM trained on the sex of the mice compare to the SVM trained on the radiation exposure type?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCdo0qpTF_AY"
      },
      "source": [
        "**Double click here to enter your answers to the questions in this text box.**\n",
        "\n",
        "<font color=\"green\" size=5>\n",
        "Answer:\n",
        "<br>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gXGLZE-YoEQH"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AMPTLH2xDOm0"
      },
      "outputs": [],
      "source": [
        "# Import libraries for CNN training and downstream analysis\n",
        "import os\n",
        "import boto3\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import tensorflow as tf\n",
        "from botocore import UNSIGNED\n",
        "from botocore.client import Config\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MdR04buNEnla"
      },
      "outputs": [],
      "source": [
        "# Split the dataset 80-20 training - validation\n",
        "train_labels = sex_labels.sample(int(0.8*len(sex_labels))) # get random 80% images and their labels\n",
        "train_paths = ['Microscopy/train/' + x for x  in list(train_labels['filename'])] # get the filepaths for those images in the same order\n",
        "\n",
        "val_labels = sex_labels[~sex_labels.isin(train_labels)].dropna() # get the other 20% of the images and their labels\n",
        "val_paths = ['Microscopy/train/' + x for x  in list(val_labels['filename'])] # and get their filepaths\n",
        "\n",
        "# Define model parameters\n",
        "batch_size = 32\n",
        "input_shape = (140, 140, 1)\n",
        "num_classes = 2\n",
        "epochs = 10\n",
        "steps_per_epoch = len(train_paths) // batch_size\n",
        "validation_steps = len(val_paths) // batch_size\n",
        "\n",
        "# Define data generators for training and validation\n",
        "def generate_batches_from_s3(object_keys, labels_df, batch_size, input_shape):\n",
        "    indexes = np.arange(len(labels_df))\n",
        "    while True:\n",
        "        for i in range(0, len(labels_df), batch_size):\n",
        "            num_rows = len(labels_df)\n",
        "            batch_indexes = indexes[i:i+batch_size]\n",
        "            X = np.empty((len(batch_indexes), *input_shape))\n",
        "            y = np.empty((len(batch_indexes)), dtype=int)\n",
        "            for j, idx in enumerate(batch_indexes):\n",
        "                row = labels_df.iloc[idx]\n",
        "                img_filename = row['filename']\n",
        "                img_label = 0 if row['sex'] == 'FEMALE' else 1\n",
        "                obj = s3_client.get_object(Bucket=s3_bucket, Key=os.path.join(s3_dir_path, img_filename))\n",
        "                img_bytes = obj['Body'].read()\n",
        "                img = Image.open(BytesIO(img_bytes))\n",
        "                img = img.resize((input_shape[0], input_shape[1]))\n",
        "                img = np.array(img)\n",
        "                img = np.clip(img, 400, 4000) # mimicking imshow vmin/vmax https://stackoverflow.com/questions/31232733/vmin-vmax-algorithm-matplotlib\n",
        "                X[j] = np.expand_dims(img, axis=-1)\n",
        "                y[j] = img_label\n",
        "            yield X, y\n",
        "\n",
        "\n",
        "# Create data generators for training and validation\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: generate_batches_from_s3(train_paths, train_labels, batch_size, input_shape),\n",
        "    output_types=(tf.float32, tf.int32),\n",
        "    output_shapes=(tf.TensorShape([None, *input_shape]), tf.TensorShape([None])),\n",
        ").repeat()\n",
        "\n",
        "validation_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: generate_batches_from_s3(val_paths, val_labels, batch_size, input_shape),\n",
        "    output_types=(tf.float32, tf.int32),\n",
        "    output_shapes=(tf.TensorShape([None, *input_shape]), tf.TensorShape([None])),\n",
        ").repeat()\n",
        "\n",
        "# Build CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_history = model.fit(train_dataset, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=validation_dataset, validation_steps=validation_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFdcw607qik0"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming `history` is the History object returned by model.fit()\n",
        "train_accuracy = model_history.history['accuracy']\n",
        "val_accuracy = model_history.history['val_accuracy']\n",
        "epochs = range(1, len(train_accuracy) + 1)\n",
        "\n",
        "plt.plot(epochs, train_accuracy, 'b', label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'r', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBJEP-ElGLmh"
      },
      "source": [
        "**CHALLENGE QUESTIONS:**\n",
        "\n",
        "1. How did the CNN trained on the sex of the mice perform compared to the CNN trained on the radiation exposure type?\n",
        "\n",
        "2. How did the CNN trained on the sex of the mice perform compared to the SVM trained on the sex of the mice?\n",
        "\n",
        "3. Based on these results, do you think the sex of the mice is being captured in the 53bp1+ DNA damage patterns of the images?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkUsS16QGLmh"
      },
      "source": [
        "**Double click here to enter your answers to the questions in this text box.**\n",
        "\n",
        "<font color=\"green\" size=5>\n",
        "Answers:\n",
        "<br>\n",
        "<ol>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "</ol>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THFXNreS28Ch"
      },
      "source": [
        "# Strain Classifier\n",
        "\n",
        "Let's return again to the questions at the beginning of this project:\n",
        "\n",
        "- Can we train a machine learning classifier to classify the *sex* of the mouse from which each cell nucleus 53bp1+ DNA damage image was taken?\n",
        "  - What about the *strain* of the mouse?\n",
        "- If not, what does this tell us about what biological traits are being captured in the radiation images?\n",
        "\n",
        "----\n",
        "\n",
        "Let's train the same SVM and CNN architectures but instead we will use the *strain* of the mouse that each cellular image came from as the labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l119t3Xf2-Lb"
      },
      "outputs": [],
      "source": [
        "# Get the strain labels only\n",
        "strain_labels = meta[['filename','strain']]\n",
        "strain_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HVoDJbCwvJKS"
      },
      "source": [
        "## SVM\n",
        "\n",
        "We will train an SVM on the strain labels below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F34RDt2CvN8m"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tqdm import tqdm\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtXsBiSovN8w"
      },
      "outputs": [],
      "source": [
        "# Encoding labels\n",
        "label_encoder = LabelEncoder()\n",
        "strain_labels['strain_type_encoded'] = label_encoder.fit_transform(strain_labels['strain'])\n",
        "\n",
        "# Load and preprocess images\n",
        "X = resized_images\n",
        "y = strain_labels['strain_type_encoded'].values\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Flatten image arrays\n",
        "X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
        "X_test_flat = X_test.reshape(X_test.shape[0], -1)\n",
        "\n",
        "# Train SVM model\n",
        "svm_model = SVC(kernel='rbf')\n",
        "svm_model.fit(X_train_flat, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = svm_model.predict(X_test_flat)\n",
        "\n",
        "# Evaluate model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=label_encoder.classes_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvflktyEvXRP"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Predictions\n",
        "y_pred = svm_model.predict(X_test_flat)\n",
        "\n",
        "# Generate confusion matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)\n",
        "plt.xlabel('Predicted Label')\n",
        "plt.ylabel('True Label')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rE7F5eDPHlMa"
      },
      "source": [
        "**CHALLENGE QUESTION:**\n",
        "\n",
        "1. How did the SVM trained on the strain of the mice perform compared to the SVMs trained on the sex of the mice and the radiation exposure type?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46lCzNp8HlMb"
      },
      "source": [
        "**Double click here to enter your answers to the questions in this text box.**\n",
        "\n",
        "<font color=\"green\" size=5>\n",
        "Answer:\n",
        "<br>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3ofr1skvKlY"
      },
      "source": [
        "## CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Isp01sCSHPbn"
      },
      "outputs": [],
      "source": [
        "# Import libraries for CNN training and downstream analysis\n",
        "import os\n",
        "import boto3\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "import tensorflow as tf\n",
        "from botocore import UNSIGNED\n",
        "from botocore.client import Config\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XEh6BdAz2-Lc"
      },
      "outputs": [],
      "source": [
        "# Split the dataset 80-20 training - validation\n",
        "train_labels = strain_labels.sample(int(0.8*len(strain_labels))) # get random 80% images and their labels\n",
        "train_paths = ['Microscopy/train/' + x for x  in list(train_labels['filename'])] # get the filepaths for those images in the same order\n",
        "\n",
        "val_labels = strain_labels[~strain_labels.isin(train_labels)].dropna() # get the other 20% of the images and their labels\n",
        "val_paths = ['Microscopy/train/' + x for x  in list(val_labels['filename'])] # and get their filepaths\n",
        "\n",
        "# Define model parameters\n",
        "batch_size = 32\n",
        "input_shape = (140, 140, 1)\n",
        "num_classes = 2\n",
        "epochs = 10\n",
        "steps_per_epoch = len(train_paths) // batch_size\n",
        "validation_steps = len(val_paths) // batch_size\n",
        "\n",
        "# Define data generators for training and validation\n",
        "def generate_batches_from_s3(object_keys, labels_df, batch_size, input_shape):\n",
        "    indexes = np.arange(len(labels_df))\n",
        "    while True:\n",
        "        for i in range(0, len(labels_df), batch_size):\n",
        "            num_rows = len(labels_df)\n",
        "            batch_indexes = indexes[i:i+batch_size]\n",
        "            X = np.empty((len(batch_indexes), *input_shape))\n",
        "            y = np.empty((len(batch_indexes)), dtype=int)\n",
        "            for j, idx in enumerate(batch_indexes):\n",
        "                row = labels_df.iloc[idx]\n",
        "                img_filename = row['filename']\n",
        "                img_label = 0 if row['strain'] == 'BALBC' else 1\n",
        "                obj = s3_client.get_object(Bucket=s3_bucket, Key=os.path.join(s3_dir_path, img_filename))\n",
        "                img_bytes = obj['Body'].read()\n",
        "                img = Image.open(BytesIO(img_bytes))\n",
        "                img = img.resize((input_shape[0], input_shape[1]))\n",
        "                img = np.array(img)\n",
        "                img = np.clip(img, 400, 4000) # mimicking imshow vmin/vmax https://stackoverflow.com/questions/31232733/vmin-vmax-algorithm-matplotlib\n",
        "                X[j] = np.expand_dims(img, axis=-1)\n",
        "                y[j] = img_label\n",
        "            yield X, y\n",
        "\n",
        "\n",
        "# Create data generators for training and validation\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: generate_batches_from_s3(train_paths, train_labels, batch_size, input_shape),\n",
        "    output_types=(tf.float32, tf.int32),\n",
        "    output_shapes=(tf.TensorShape([None, *input_shape]), tf.TensorShape([None])),\n",
        ").repeat()\n",
        "\n",
        "validation_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: generate_batches_from_s3(val_paths, val_labels, batch_size, input_shape),\n",
        "    output_types=(tf.float32, tf.int32),\n",
        "    output_shapes=(tf.TensorShape([None, *input_shape]), tf.TensorShape([None])),\n",
        ").repeat()\n",
        "\n",
        "# Build CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(128, (3, 3), activation='relu'),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model_history = model.fit(train_dataset, epochs=epochs, steps_per_epoch=steps_per_epoch, validation_data=validation_dataset, validation_steps=validation_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFA8QceW2-Lc"
      },
      "outputs": [],
      "source": [
        "train_accuracy = model_history.history['accuracy']\n",
        "val_accuracy = model_history.history['val_accuracy']\n",
        "epochs = range(1, len(train_accuracy) + 1)\n",
        "\n",
        "plt.plot(epochs, train_accuracy, 'b', label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'r', label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKWNlVhNH9xi"
      },
      "source": [
        "**CHALLENGE QUESTIONS:**\n",
        "\n",
        "1. How did the CNN trained on the strain of the mice perform compared to the CNNs trained on the sex of the mice and the radiation exposure type?\n",
        "\n",
        "2. How did the CNN trained on the strain of the mice perform compared to the SVM trained on the strain of the mice?\n",
        "\n",
        "3. Based on the performance of each classifier, which aspect(s) of the 53bp1+ DNA damage images do you think the strain of the mice is affecting?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gH-rnB4eH9xs"
      },
      "source": [
        "**Double click here to enter your answers to the questions in this text box.**\n",
        "\n",
        "<font color=\"green\" size=5>\n",
        "Answers:\n",
        "<br>\n",
        "<ol>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "<br>\n",
        "<li>\n",
        "\n",
        "</li>\n",
        "</ol>\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyT1o4iHhrR4"
      },
      "source": [
        "# Choose Your Own Adventure (Optional)\n",
        "\n",
        "Now that we have answered the questions we set out at the beginning of this project, there are multiple ways you could take this project further if you are interested in writing some of your own code.\n",
        "\n",
        "- Transfer learning (start with a pretrained image model)\n",
        "- Segment DNA damage foci\n",
        "- xAI - SHAP, LIME, Captum, attention maps\n",
        "- Regression to predict nfoci"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8RXLQR_IqFI"
      },
      "source": [
        "## Transfer Learning\n",
        "\n",
        "In the sections above, we trained an SVM and a CNN from scratch. However, in some cases, better performance has been shown for image classification when using a pretrained model.\n",
        "\n",
        "Several official Tensorflow pretrained models for image classification are available through GitHub: https://github.com/tensorflow/models/tree/master/official#image-classification\n",
        "\n",
        "There are also hundreds of pretrained image classification models on HuggingFace: https://huggingface.co/models?sort=trending&search=image+classification\n",
        "\n",
        "Feel free to load one of these models and test it out on this dataset to see if you can improve performance!\n",
        "\n",
        "> Don't forget to evaluate which images are being misclassified and why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfjDa9DjJwSp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pWwEFao3h57L"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIZYqmm4JxNr"
      },
      "source": [
        "## Segmentation\n",
        "\n",
        "In the sections above, we relied on each model identify important features from the images, such as the fluorescent foci that indicate DNA damage.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1OS3kJDg88GPwZeFcCEKtr3eDANj4nBw_\">\n",
        "\n",
        "However, an auto-segmentation model would be useful to identify and quantify the number of distinct DNA damage foci per image, in each class.\n",
        "\n",
        "Once again, several image segmentation models are available on Tensorflow GitHub or on HuggingFace:\n",
        "\n",
        "https://github.com/tensorflow/models/blob/master/official/vision/README.md#object-detection-and-instance-segmentation\n",
        "\n",
        "https://huggingface.co/models?sort=trending&search=image+segmentation\n",
        "\n",
        "You could also train your own image segmentation model: https://www.tensorflow.org/tutorials/images/segmentation\n",
        "\n",
        "> The end result of this effort should be a quantification of the number of distinct DNA damage fluorescent foci per image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWKJlIB6KfjO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p_rpGUcKoxa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGHZ_1brKpVx"
      },
      "source": [
        "## Explainable AI (xAI) for CNN\n",
        "\n",
        "In the sections above, we tried to understand which features the CNN was paying attention to, by identifying the misclassified images and applying our knowledge of biology.\n",
        "\n",
        "There are several Python libraries which are specifically designed to identify the areas of an image that a CNN is using for classification.\n",
        "\n",
        "It would be very useful to implement one of these libraries to understand which areas of the image the CNN is focused on. We can then make a biological interpretation.\n",
        "\n",
        "GradCAM: https://keras.io/examples/vision/grad_cam/\n",
        "\n",
        "SHAP: https://shap.readthedocs.io/en/latest/image_examples.html\n",
        "\n",
        "Feel free to explore on your own and identify other useful libraries!\n",
        "\n",
        "> The end result of this effort should be code that creates a heatmap overlaied on one of the DNA damage microscopy images, where the colors of the heatmap indicate the areas that were used by the CNN for classification.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aj-I14C02kd2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDbUinC22l_b"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}